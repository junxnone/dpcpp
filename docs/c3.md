# 3 数据管理
\begin{center}
	\includegraphics[width=0.5\textwidth]{content/chapter-3/images/1}
\end{center}

超级计算机的架构师们经常哀叹，我们需要“喂养野兽”。“喂养野兽”指的是，向大量并行的计算机的“野兽”喂数据。\par

在异构机器上进行数据并行，需要确保在需要时数据出现在正确的地方。大程序中，会有很多工作，整理如何管理所有需要的数据移动可能是一场噩梦。\par

我们将解释管理数据的两种方法:统一共享内存(Unified Shared Memory, USM)和缓冲区。USM基于指针，C++开发者对它很熟悉。缓冲区提供了更高级别的抽象。\par

我们需要控制数据的移动，本章将介绍实现该目标的方式。\par

第2章中，我们研究了如何控制代码的执行位置。代码需要数据作为输入，生成数据作为输出。因为代码可能运行在多个设备上，而这些设备不一定共享内存，所以需要管理数据移动。即使共享数据，比如USM，同步和一致性也需要管理。\par

一个问题是“为什么编译器不自动地做所有的事情?”，编译器可以处理很多事情，但如果不把自己定位为开发者，性能通常是次优的，所以编译器没必要做所有的事情。在实践中，为了获得最佳性能，编写异构程序时，需要关注代码执行(第2章)和数据移动(本章)。\par

本章提供了管理数据的介绍，包括数据使用的顺序。前一章展示了如何控制代码的运行位置，本章将让数据在我们要求代码执行的地方出现，这不仅对正确执行应用程序很重要，而且也可以最小化执行时间和功耗。\par


## 3.1 介绍
没数据，不计算。\par

加速计算的意义在于更快地得出答案，数据并行计算最重要的是如何访问数据，而在机器中引入加速器设备将使情况复杂化。传统的单CPU系统中，只有一个内存。加速器设备通常有自己的内存，这些内存主机不能直接访问。因此，支持独立设备的并行编程模型必须管理这些多内存，以及移动数据的机制。\par

本章中，我们将概述数据管理的各种机制。引入统一共享内存和用于数据管理的缓冲区，并描述了内核执行和数据移动之间的关系。\par

## 3.2 数据管理的问题
用于并行编程的共享内存模型的优点是提供单一、共享的内存。我们不需要显式的从并行任务中访问内存(除了适当的同步以避免数据竞争)。当某些类型的加速器设备(例如集成GPU)与主机CPU共享内存时，许多独立加速器有自己的内存，不与CPU的内存在一起，如图3-1所示。\par

\hspace*{\fill} \par %插入空行
图3-1 多个独立的内存
\begin{center}
	\includegraphics[width=0.8\textwidth]{content/chapter-3/images/2}
\end{center}


## 3.3 本地设备和远程设备
运行在设备上的程序直接使用到设备上的内存，要比远程内存读写数据时性能会更好。我们将直接访问独立内存称为本地访存，访问其他设备的内存是远程访存。远程访存往往比本地访存慢，因为需要以更低的带宽和/或更高的延迟在数据链路上进行传输，所以将计算和数据放在一起有利与计算。为了实现这一点，必须以某种方式确保数据在不同的内存间可以复制或迁移，以便将其移动到更接近计算发生的位置。\par

\hspace*{\fill} \par %插入空行
图3-2 数据移动和内核执行
\begin{center}
	\includegraphics[width=0.8\textwidth]{content/chapter-3/images/3}
\end{center}

## 3.4 管理多种内存
管理多种内存可以通过两种方式来实现:显式地通过程序实现，或隐式地通过运行时实现。每种方法都有其优点和缺点，可以根据情况或个人喜好进行选择。\par

\hspace*{\fill} \par %插入空行
\textbf{显式数据移动}

显式地在不同内存之间复制数据。图3-2展示了一个有独立加速器的系统，必须先将内核需要的任何数据从主机内存复制到GPU内存。在内核计算结果之后，将这些结果复制回CPU，然后主机才能使用这些数据。\par

显式数据移动的优点是，可以完全控制数据在不同内存之间的传输时间。因为要在某些硬件上获得最佳性能，将计算与数据传输重叠必不可少。\par

显式数据移动的缺点是，指定所有数据移动会很繁琐，而且容易出错。传输不正确的数据量，或者没有确保在内核开始计算之前已经传输了所有数据，都可能导致不正确的结果。从一开始就正确地移动所有数据是非常耗时的任务。\par

\hspace*{\fill} \par %插入空行
\textbf{隐式数据移动}

程序控制的显式数据移动的替代方案，由运行时或驱动程序控制的隐式数据移动。这种情况下，运行时不需要进行显式复制，而是负责确保数据在使用之前就传输到适当的位置。\par

隐式数据移动的优点是，应用程序直接连接到设备内存，这样会更快，所有工作都由运行时完成。这也减少了引入错误的机会，因为运行时将自动识别何时执行数据传输，以及传输多少数据。\par

隐式数据移动的缺点是，对运行时的隐式行为控制较少或没有控制。运行时将提供功能的正确性，但可能不会以最佳的方式移动数据，以确保计算与数据传输重叠，这可能会对程序性能产生负面影响。\par

\hspace*{\fill} \par %插入空行
\textbf{选择正确的策略}

选择最佳策略取决于许多的因素，不同的策略可能适合程序开发的不同阶段。我们甚至可以决定，最好的解决方案可以为程序的不同部分混合和适配显式和隐式方法。可以选择使用隐式数据移动，来简化移植到新设备的过程。当开始调优程序性能时，可能会用代码中对性能至关重要的显式部分替换隐式数据移动。未来的章节将涵盖数据传输如何与计算重叠，从而达到优化性能的目的。\par


## 3.5 统一共享内存、内存和图像
有三种管理内存的方法:统一共享内存(USM)、缓冲区和图像。\par

USM基于指针，C/C++开发者应该很熟悉。USM的优点是更容易与现有C++代码集成。\par

缓冲区(由缓冲区模板类表示)描述1、2或3维数组，提供了可以在主机或设备上访问的内存。缓冲区不直接由程序访问，而是通过访问器使用。\par

图像作为一种特殊的缓冲区，提供特定于图像处理的功能。这个功能包括支持特殊的图像格式，使用采样器读取图像等。缓冲区和图像是许多问题的解决方法，但现有代码中重写所有接口来使用缓冲区或访问器可能非常耗时。由于缓冲区和图像的接口基本上是相同的，本章剩下的部分只关注USM和缓冲区。\par


## 3.6 统一共享内存

USM是可供使用的数据管理工具。当移植大量使用指针的工程时，USM可以简化工作。支持USM的设备支持统一虚拟地址空间，拥有统一虚拟地址空间意味着主机上的USM返回的指针都是设备上的有效指针。不需要手动转换主机指针来获得“设备指针”——可以在主机和设备上看到相同的指针。\par

关于USM更详细的解读会在第6章继续。\par

\hspace*{\fill} \par %插入空行
\textbf{通过指针访问内存}

当系统包含主机内存和设备内存时，不是所有的内存都相同，所以USM定义了三种不同的分配类型:设备、主机和共享。所有类型的分配都在主机上执行。图3-3总结了各分配类型的特点。\par

\hspace*{\fill} \par %插入空行
图3-3 USM分配类型
\begin{table}[H]
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		\textbf{分配类型} & \textbf{描述}                       & \textbf{可访问主机?} & \textbf{可访问设备?} & \textbf{位于}        \\ \hline
		\textbf{设备}          & 设备内存的分配               & \XSolidBrush                       & \Checkmark                              & 设备                     \\ \hline
		\textbf{主机}            & 主机内存的分配                & \Checkmark                            & \Checkmark                               & 主机                       \\ \hline
		\textbf{共享}          & 主机和设备共享 & \Checkmark                            & \Checkmark                               & 随意迁移 \\ \hline
	\end{tabular}
\end{table}

设备分配发生在设备内存中，分配的内存可以从设备上读取和写入，但不能直接从主机上访问。必须使用显式的复制操作，在主机内存和设备内存之间移动数据。\par

主机和设备上都可以访问主机内存，这意味着相同的指针在主机代码和设备内核中都有效。然而，访问这样的指针时，数据总是来自主机内存。当访问设备时，数据不会从主机迁移到设备内存。相反，数据通常通过总线发送，例如PCI-Express (PCI-E)将设备连接到主机。\par

共享分配的内存在主机和设备上都可以访问，非常类似于主机分配，不同之处在于数据可以在主机内存和设备本地内存之间迁移。迁移之后，对设备的访问将在设备内存中进行，而不是远程访问主机内存。通常，这是通过运行时内部的机制和底层驱动实现。\par

\hspace*{\fill} \par %插入空行
\textbf{USM和数据移动}

USM支持显式和隐式的数据移动策略，不同的分配类型对应不同的策略。设备内存要求显式地在主机和设备之间移动数据，而主机和共享内存提供隐式的数据移动。\par

\hspace*{\fill} \par %插入空行
\textbf{USM显式数据移动}

使用USM的显式数据移动，通过设备内存和在队列和处理程序中使用特殊的memcpy()完成的。将memcpy()操作(动作)放入队列，将数据从主机传输到设备，或从设备传输到主机。\par

图3-4包含操作设备分配的内核。内核执行前后使用memcpy()操作，hostArray和deviceArray之间复制数据。队列上调用wait()，确保在内核执行之前复制到设备的操作已经完成，并确保数据复制回主机之前内核已经完成。我们将在本章后面学习如何消除这些调用。\par

\hspace*{\fill} \par %插入空行
图3-4 显式地数据移动
\begin{lstlisting}[caption={}]
#include <CL/sycl.hpp>
#include<array>
using namespace sycl;
constexpr int N = 42; 

int main() {
	queue Q;
	
	std::array<int,N> host_array;
	int *device_array = malloc_device<int>(N, Q);
	
	for (int i = 0; i < N; i++)
		host_array[i] = N;
		
	// We will learn how to simplify this example later
	Q.submit([&](handler &h) {
		// copy hostArray to deviceArray
		h.memcpy(device_array, &host_array[0], N * sizeof(int));
	});
	Q.wait();
	
	
	Q.submit([&](handler &h) {
		h.parallel_for(N, [=](id<1> i) { device_array[i]++; }); 
	});
	Q.wait();
	
	Q.submit([&](handler &h) {
		// copy deviceArray back to hostArray
		h.memcpy(&host_array[0], device_array, N * sizeof(int)); 
	});
	Q.wait();
	
	free(device_array, Q);
	return 0;
}
\end{lstlisting}

\hspace*{\fill} \par %插入空行
\textbf{USM隐式数据移动}

使用USM的隐式数据移动是通过主机和共享内存完成的。使用这类型的内存，不需要显式地插入复制操作。相反，只需访问内核中的指针，任何数据移动都可以自动执行，无需手动干预(只要设备支持这些内存分配)。这提高了代码的移植性:只需用适当的USM分配函数，替换malloc或new(以及free)，一切就好了。\par

\hspace*{\fill} \par %插入空行
图3-5 USM隐式地数据移动
\begin{lstlisting}[caption={}]
#include <CL/sycl.hpp>
using namespace sycl;
constexpr int N = 42;

int main() {
	queue Q;
	int *host_array = malloc_host<int>(N, Q);
	int *shared_array = malloc_shared<int>(N, Q);
	
	for (int i = 0; i < N; i++) {
		// Initialize hostArray on host
		host_array[i] = i;
	}

	// We will learn how to simplify this example later
	Q.submit([&](handler &h) {
		h.parallel_for(N, [=](id<1> i) {
			// access sharedArray and hostArray on device
			shared_array[i] = host_array[i] + 1;
		});
	});
	Q.wait();
	
	for (int i = 0; i < N; i++) {
		// access sharedArray on host
		host_array[i] = shared_array[i];
	}

	free(shared_array, Q);
	free(host_array, Q);
	return 0;
}
\end{lstlisting}

图3-5中，创建了两个数组，hostArray和sharedArray，分别是主机内存和共享内存。虽然主机和共享内存都可以在主机代码中访问，但这里只初始化了hostArray。类似地，可以在内核直接访问，执行远程读取数据。运行时确保sharedArray在内核访问之前，数据在设备上是可用的，并且在之后主机代码读取时会回移数据，这些都不需要开发者干预。\par


## 3.7 内存
为数据管理提供的另一个方法的是缓冲区对象。缓冲区是一种数据抽象，表示给定C++类型的一个或多个对象。缓冲区对象可以是标量数据类型(如int、float或double)、向量数据类型(第11章)或用户定义的类或结构。缓冲区中的数据结构必须可复制，这意味着可以安全地逐个字节地复制对象，而不需要调用复制构造函数。\par

虽然缓冲区本身是单个实例，但缓冲区封装的C++类型可以是包含多个对象的数组。缓冲区代表的是数据对象，而不是特定的内存地址，所以不能像常规C++数组那样直接访问。实际上，出于性能原因，缓冲区对象可能映射到多个不同设备上的多个不同内存位置，甚至是同一个设备上的多个内存位置。而我们只能使用访问器，来读取和写入缓冲区。\par

第7章有更多关于缓冲区的描述。\par

\hspace*{\fill} \par %插入空行
\textbf{创建缓冲区}

可以通过多种方式创建缓冲区。最简单的方法是构造新的缓冲区，指定缓冲区的大小。然而，以这种方式创建的缓冲区并不会初始化数据，试图从缓冲区中读取数据之前，必须通过其他方式初始化缓冲区。\par

还可以以主机上的现有数据创建缓冲区。通过调用构造函数来实现，构造函数接受指向主机的内存指针、一组输入迭代器或具有某些属性的容器。构造缓冲区的过程中，数据从现有的主机内存复制到缓冲区对象的主机内存中。如果在OpenCL中使用SYCL互操作特性，也可以使用现有的cl\_mem实例创建缓冲区。\par

\hspace*{\fill} \par %插入空行
\textbf{访问缓存}

主机和设备不能直接访问缓冲区(除非通过这里没有描述的高级和不经常使用的机制)，而必须创建访问器来读取和写入缓冲区。访问器向运行时提供如何使用缓冲区中的数据信息，从而允许运行时正确地进行数据移动。\par

\hspace*{\fill} \par %插入空行
图3-6 缓冲区和访问器
\begin{lstlisting}[caption={}]
#include <CL/sycl.hpp>
#include <array>
using namespace sycl;
constexpr int N = 42;

int main() {
	std::array<int,N> my_data;
	for (int i = 0; i < N; i++)
		my_data[i] = 0;
	
	{
		queue q;
		buffer my_buffer(my_data);
		
		q.submit([&](handler &h) {
			// create an accessor to update
			// the buffer on the device
			accessor my_accessor(my_buffer, h);
			
			h.parallel_for(N, [=](id<1> i) {
				my_accessor[i]++;
			});
		});
	
		// create host accessor
		host_accessor host_accessor(my_buffer);
		for (int i = 0; i < N; i++) {
			// access myBuffer on host
			std::cout << host_accessor[i] << " ";
		}
		std::cout << "\n";
	}

	// myData is updated when myBuffer is
	// destroyed upon exiting scope
	for (int i = 0; i < N; i++) {
		std::cout << my_data[i] << " ";
	}
	std::cout << "\n";
}
\end{lstlisting}

\hspace*{\fill} \par %插入空行
图3-7 缓冲区的访问模式
\begin{table}[H]
	\begin{tabular}{|l|l|}
		\hline
		\textbf{访问模式} & \textbf{描述}                                \\ \hline
		\textbf{read}        & 只能读                                   \\ \hline
		\textbf{write}       & 只写访问(以前的内容不丢弃) \\ \hline
		\textbf{read\_write} & 可读写              \\ \hline
	\end{tabular}
\end{table}

\hspace*{\fill} \par %插入空行
\textbf{访问模式}

创建访问器时，可以通知运行时来提供更多的优化信息，可以通过指定访问模式来实现这一点。图3-7中的Access::mode enum中定义了访问模式。图3-6所示的代码中，访问器myAccessor以默认的访问模式创建，access::mode::read\_write，这让运行时知道我们打算通过myAccessor对缓冲区进行读写操作。访问模式是运行时优化隐式数据移动的方式，例如：使用access::mode::read会在内核开始执行之前，需要数据在设备上可用。如果内核只通过访问器读取数据，在内核完成后就不需要将数据复制回主机。同样，access::mode::write让运行时知道我们将修改缓冲区的内容，并且需要在计算结束后将结果复制回来。\par

创建适当模式的访问器，可以向运行时提供更多关于如何在程序中使用数据的信息。运行时使用访问器对数据的使用进行排序，也可以使用这些数据来优化内核调度和数据移动。第7章中，将继续讨论访问模式和优化标记。\par


## 3.8 对数据进行排序
内核可以视为提交执行的异步任务。这些任务必须提交到一个队列中，并安排在设备上执行。许多情况下，内核必须按照特定的顺序执行，才能计算出正确的结果。如果获得正确的结果需要任务A在任务B之前执行，那么任务A和B之间存在依赖关系。\par
 
然而，内核并不是调度的唯一形式。内核开始执行前，内核访问的任何数据都需要在设备上可用。这些数据依赖可以以数据任务的方式，从一个设备传输到另一个设备。数据传输任务可以是显式编码的拷贝操作，也可以是运行时执行的隐式数据移动。\par

如果把程序中的所有任务，以及存在的依赖关系画出来，就可以形成一个图。这个任务图是个有向无环图(DAG)，其中节点是任务，边是依赖项。图是有向的，因为依赖是单向的:任务A必须发生在任务B之前。因为不包含从节点返回自身的循环或路径，所以无环。\par

图3-8中，A任务前必须执行任务B和C。同样地，B和C之前必须在D之前执行。而B和C没有依赖，运行时是可以以任何顺序执行(甚至并行)任务。因此，如果B和C能够同时执行，则这个图可能的法律次序是: A $\Rightarrow$ B $\Rightarrow$ C $\Rightarrow$ D, A $\Rightarrow$ C $\Rightarrow$ B $\Rightarrow$ D，甚至时A $\Rightarrow$ \{B,C\} $\Rightarrow$ D。\par

\hspace*{\fill} \par %插入空行
图3-8 简单的任务图
\begin{center}
	\includegraphics[width=0.6\textwidth]{content/chapter-3/images/4}
\end{center}

任务可能与所有任务的子集有依赖性，我们只指定与正确性有关的依赖项。这种灵活性为优化任务图的执行顺序提供了自由度。图3-9中，我们扩展了任务图图3-8，添加了E和F，并且E必须在F前执行。然而，任务E和F与节点A,B,C,D没有依赖性。这允许运行时可以选择序执行任务的顺序。\par

\hspace*{\fill} \par %插入空行
图3-9 不相交依赖关系的任务图
\begin{center}
	\includegraphics[width=0.8\textwidth]{content/chapter-3/images/5}
\end{center}

有两种不同的方法来为任务的执行(例如内核的启动)建模:队列可以按照提交的顺序执行任务，也可以按照自定义的依赖项的任意顺序执行任务。我们有几种机制来定义正确排序所需的依赖项。\par

\hspace*{\fill} \par %插入空行
\textbf{有序队列}

对任务进行排序的最简单方式是将它们提交给一个有序的队列对象。有序队列按照任务提交的顺序执行任务，如图3-10所示。尽管有序队列的任务排序非常简单，它的缺点是即使独立任务之间不存在依赖关系，任务的执行也将串行化。有序队列在启动应用程序时很有用，因为简单、直观、执行顺序确定，并且适用于许多代码。\par

\hspace*{\fill} \par %插入空行
图3-10 有序队列的使用方式
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-3/images/6}
\end{center}

\hspace*{\fill} \par %插入空行
\textbf{无序队列}

由于队列是无序队列(除非使用有序队列属性创建)，必须提供提交任务进行的排序方法。队列允许通知运行时任务间的依赖关系，从而对任务进行排序。并且，可以使用命令组显式或隐式地指定这些依赖项。\par

命令组是指定任务及其依赖关系的对象。命令组通常以C++ Lambda的形式，作为参数传递给队列对象的submit()。这里Lambda的唯一参数是对handler对象的引用。handler对象在命令组中用于指定操作、创建访问器和指定依赖项。\par

\hspace*{\fill} \par %插入空行
\textbf{事件的显式依赖}

任务之间的显式依赖关系就像我们已经看到的例子(图3-8)，其中任务A必须在任务B之前执行，通过这种方式表达的依赖关系是显式的，并且基于计算，而不是数据。注意，表达计算之间的依赖关系，主要与使用USM有关，使用缓冲区的代码通过访问器表达大多数依赖关系。图3-4和图3-5中，只是告诉队列等待之前提交的所有任务完成后再继续，而我们可以通过事件对象表达任务的依赖关系。当向队列提交命令组时，submit()方法会返回一个事件对象，事件可以以两种方式使用。\par

首先，可以通过在事件上显式地调用wait()来进行同步。这迫使运行时等待生成事件的任务完成，才继续执行主机程序。显式地等待事件对于调试应用程序非常有用，但wait()会过度地限制任务的异步执行，因为阻塞主机线程上的所有执行，也可以在队列对象上调用wait()，这将阻塞主机上的执行，直到所有进入队列的任务都完成。\par

这就引出了使用事件的第二种方式。处理程序类包含一个名为depends\_on()的方法。此方法接受单个事件或事件组，并通知运行时所提交的命令组需要在执行命令组内的操作之前完成指定的事件。图3-11展示了如何使用depends\_on()来排序任务。\par

\hspace*{\fill} \par %插入空行
图3-11 使用事件和depends\_on
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-3/images/7}
\end{center}

\hspace*{\fill} \par %插入空行
\textbf{访问器的隐式依赖}

任务之间的隐式依赖关系由数据依赖关系创建，任务之间的数据依赖有三种形式，如图3-12所示。\par

\hspace*{\fill} \par %插入空行
图3-12 三种的数据依赖关系
\begin{table}[H]
	\begin{tabular}{|c|c|}
		\hline
		\textbf{依赖类型}                                                  & \textbf{描述}                                                  \\ \hline
		\textbf{\begin{tabular}[c]{@{}c@{}}Read-after-Write\\ (RAW)\end{tabular}} & 任务B需要读取任务A计算的数据             \\ \hline
		\textbf{\begin{tabular}[c]{@{}c@{}}Write-after-Read\\ (WAR)\end{tabular}} & 任务A读取数据之后，任务B写入数据 \\ \hline
		\textbf{Write-afterWrite(WAW)}                                            & 任务A写入的数据后，任务B写入数据          \\ \hline
	\end{tabular}
\end{table}

数据依赖关系以两种方式表示:访问器和执行顺序。两者都必须用于运行时，以正确计算数据依赖关系。如图3-13和3-14所示。\par

\hspace*{\fill} \par %插入空行
图3-13 读后写
\begin{lstlisting}[caption={}]
#include <CL/sycl.hpp>
#include <array>
using namespace sycl;
constexpr int N = 42;

int main() {
	std::array<int,N> a, b, c;
	for (int i = 0; i < N; i++) {
		a[i] = b[i] = c[i] = 0;
	}

	queue Q;
	
	// We will learn how to simplify this example later
	buffer A{a};
	buffer B{b};
	buffer C{c};
	
	Q.submit([&](handler &h) {
		accessor accA(A, h, read_only);
		accessor accB(B, h, write_only);
		h.parallel_for( // computeB
		N,
		[=](id<1> i) { accB[i] = accA[i] + 1; });
	});

	Q.submit([&](handler &h) {
		accessor accA(A, h, read_only);
		h.parallel_for( // readA
		N,
		[=](id<1> i) {
			// Useful only as an example
			int data = accA[i];
		});
	});

	Q.submit([&](handler &h) {
		// RAW of buffer B
		accessor accB(B, h, read_only);
		accessor accC(C, h, write_only);
		h.parallel_for( // computeC
		N,
		[=](id<1> i) { accC[i] = accB[i] + 2; });
	});

	// read C on host
	host_accessor host_accC(C, read_only);
	for (int i = 0; i < N; i++) {
		std::cout << host_accC[i] << " ";
	}
	std::cout << "\n";
	return 0;
}
\end{lstlisting}

\hspace*{\fill} \par %插入空行
图3-14 原始任务图
\begin{center}
	\includegraphics[width=0.6\textwidth]{content/chapter-3/images/8}
\end{center}

图3-13和图3-14中，我们执行三个内核——computeb、readA和computec——然后在主机上读取最终结果。内核computeB的命令组创建两个访问器，accA和accB。这些访问器使用访问标记read\_only和write\_only进行优化，以指定不使用默认的访问模式access::mode::read\_write。内核computeB读取缓冲区A并写入缓冲区B，缓冲区A必须在内核开始执行之前从主机复制到设备上。\par

内核readA也为缓冲区A创建一个只读访问器。因为内核readA是在内核computeB后提交的，所以这会创建一个读后读的场景。然而，读后读并没有对运行时进行限制，内核可以自由地以任何顺序执行。事实上，运行时更喜欢在内核computeB之前执行内核readA，甚至同时执行。两者都需要将缓冲区A复制到设备上，但是内核computeB也需要复制缓冲区B，以确保computeB的数据覆盖相应的缓冲区。当缓冲区B的数据传输时，运行时可以执行内核读取，即使内核只会写入缓冲区，缓冲区的原始内容仍然可移动到设备上，因为不能保证缓冲区中的所有值都由内核修改(参见第7章，关于优化标记)。\par

内核computeC读取缓冲区B，这是内核computeB计算的结果。在提交内核computeB之后，提交了内核computeC，这样内核computeC对缓冲区B有数据依赖。数据依赖也称为真依赖或流依赖，因为数据需要从计算流到另一个计算，从而计算出正确的结果。因为主机希望在内核完成后读取C，所以还在内核computeC和主机之间创建了对缓冲区C的依赖，这迫使运行时将缓冲区C复制回主机。由于设备上没有对缓冲区A的写操作，因为主机已经有了最新的数据副本，所以运行时不需要将缓冲区复制回主机。\par

\hspace*{\fill} \par %插入空行
图3-15 写后读和写后写
\begin{lstlisting}[caption={}]
#include <CL/sycl.hpp>
#include <array>
using namespace sycl;
constexpr int N = 42;

int main() {
	std::array<int,N> a, b;
	for (int i = 0; i < N; i++) {
		a[i] = b[i] = 0;
	}

	queue Q;
	buffer A{a};
	buffer B{b};
	
	Q.submit([&](handler &h) {
		accessor accA(A, h, read_only);
		accessor accB(B, h, write_only);
		h.parallel_for( // computeB
		N, [=](id<1> i) {
			accB[i] = accA[i] + 1;
		});
	});

	Q.submit([&](handler &h) {
		// WAR of buffer A
		accessor accA(A, h, write_only);
		h.parallel_for( // rewriteA
		N, [=](id<1> i) {
			accA[i] = 21 + 21;
		});
	});

	Q.submit([&](handler &h) {
		// WAW of buffer B
		accessor accB(B, h, write_only);
		h.parallel_for( // rewriteB
		N, [=](id<1> i) {
			accB[i] = 30 + 12;
		});
	});

	host_accessor host_accA(A, read_only);
	host_accessor host_accB(B, read_only);
	for (int i = 0; i < N; i++) {
		std::cout << host_accA[i] << " " << host_accB[i] << " ";
	}
	std::cout << "\n";
	return 0;
}
\end{lstlisting}

\hspace*{\fill} \par %插入空行
图3-16 写后读和写后写的任务图
\begin{center}
	\includegraphics[width=0.6\textwidth]{content/chapter-3/images/9}
\end{center}

图3-15和3-16中，再次执行三个内核:computeB、rewriteA和rewriteB。内核rewriteA写缓冲区A，内核rewriteB写缓冲区B。内核rewriteA理论上可以比内核rewriteB更早执行，在内核准备好之前需要传输的数据更少。但必须等到内核computeB完成后，因为有一个写后读依赖于缓冲区A。\par

这个例子中，内核computeB需要从主机获取A的原始值，如果内核rewriteA在内核computeB之前执行，那将读取错误的值。写后读依赖也称为反依赖，原始依赖关系确保数据正确地流向正确的方向，而写后读依赖关系确保在读取现有值之前不会覆盖。内核重写函数中，写后写对缓冲区B的依赖与此类似。如果在内核computeB和rewriteB之间提交了任何对缓冲区B的读取，将形成读后写和写后读的依赖关系，从而正确地排序任务。然而，内核rewriteB和主机之间存在隐式的依赖关系，最终的数据必须写回主机。写后写依赖关系，也称为输出依赖关系，确保最终的输出数据在主机上的正确性。\par


## 3.9 选择管理策略

为程序选择正确的数据管理策略很大程度上是偏好的问题。事实上，可以从一种策略开始，然后随着项目的成熟而转向另一种策略。这里有一些指导方针，可以帮助我们选择符合需要的策略。\par

首先是使用显式数据移动还是隐式数据移动，这极大地影响了对程序进行的操作。隐式数据移动通常更容易，因为所有数据移动都是隐式处理的，从而让我们专注于计算。\par

如果从一开始就完全控制所有的数据移动，使用USM设备分配的显式数据移动就是不错的选择。我们只需要确保在主机和设备之间添加所有必要的副本即可。\par

选择隐式数据移动策略时，仍然可以选择是否使用缓冲区或USM主机或共享指针。如果正在移植一个使用指针的C/C++程序，USM是更简单的方式，无需修改大多数的代码。如果数据表示没有引导我们选择一个策略，则可以问的另一个问题，希望如何表达内核之间的依赖关系。如果更愿意考虑内核之间的数据依赖关系，那么选择缓冲区。如果倾向于把依赖关系看作是在另一个计算之前执行一个计算，并且想使用一个有序队列或显式事件或内核之间的等待来表示依赖关系，那么选择USM。\par

使用USM指针(显式或隐式数据移动)时，可以选择想要使用类型的队列。有序队列简单直观，但限制了运行时，并可能限制性能。无序队列更复杂，但给了运行时更多的自由来重新排序和重叠执行。如果程序在内核之间有复杂的依赖关系，那么无序队列类是正确的选择。如果程序只是一个接一个地运行多个内核，那么有序队列将是更好的选择。\par

## 3.10 句柄类：关键成员

我们已经展示了许多使用handler类的方法。图3-17和图3-18更详细地解释了这个非常重要的类的关键成员。我们目前还没有使用所有的成员，后续会对它们进行使用。\par

另一个queue类在第2章的末尾也有类似的解释，在线oneAPI DPC++语言手册提供了对这两个类更详细的解释。\par

\hspace*{\fill} \par %插入空行
图3-17 简化定义handler类的非访问器成员
\begin{lstlisting}[caption={}]
class handler {
	...
	// Specifies event(s) that must be complete before the action
	// defined in this command group executes.
	void depends_on({event / std::vector<event> & });
	
	// Enqueues a memset operation on the specified pointer.
	// Writes the first byte of Value into Count bytes.
	// Returns an event representing this operation.
	event memset(void *Ptr, int Value, size_t Count);
	
	// Enqueues a memcpy from Src to Dest.
	// Count bytes are copied.
	// Returns an event representing this operation.
	event memcpy(void *Dest, const void *Src, size_t Count);
	
	// Submits a kernel of one work-item for execution.
	// Returns an event representing this operation.
	template <typename KernelName, typename KernelType>
	event single_task(KernelType KernelFunc);
	
	// Submits a kernel with NumWork-items work-items for execution.
	// Returns an event representing this operation.
	template <typename KernelName, typename KernelType, int Dims>
	event parallel_for(range<Dims> NumWork-items, KernelType KernelFunc);
	
	// Submits a kernel for execution over the supplied nd_range.
	// Returns an event representing this operation.
	template <typename KernelName, typename KernelType, int Dims>
	event parallel_for(nd_range<Dims> ExecutionRange, KernelType KernelFunc);
	...
};
\end{lstlisting}


\hspace*{\fill} \par %插入空行
图3-18 简化定义handler类的访问器成员
\begin{lstlisting}[caption={}]
class handler {
	...
	// Specifies event(s) that must be complete before the action
	// Copy to/from an accessor.
	// Valid combinations:
	// Src: accessor, Dest: shared_ptr
	// Src: accessor, Dest: pointer
	// Src: shared_ptr Dest: accessor
	// Src: pointer Dest: accessor
	// Src: accesssor Dest: accessor
	template <typename T_Src, typename T_Dst, 
			int Dims, access::mode AccessMode,
			access::target AccessTarget,
			access::placeholder IsPlaceholder = 
			access::placeholder::false_t>
	void copy(accessor<T_Src, Dims, AccessMode, 
			AccessTarget, IsPlaceholder> Src,
			shared_ptr_class<T_Dst> Dst);
	void copy(shared_ptr_class<T_Src> Src,
			accessor<T_Dst, Dims, AccessMode, 
				AccessTarget, IsPlaceholder> Dst);
	void copy(accessor<T_Src, Dims, AccessMode, 
				AccessTarget, IsPlaceholder> Src,
				T_Dst *Dst);
	void copy(const T_Src *Src,
			accessor<T_Dst, Dims, AccessMode, 
				AccessTarget, IsPlaceholder> Dst);
	template <
		typename T_Src, int Dims_Src, 
		access::mode AccessMode_Src,
		access::target AccessTarget_Src, 
		typename T_Dst, int Dims_Dst,
		access::mode AccessMode_Dst, 
		access::target AccessTarget_Dst,
		access::placeholder IsPlaceholder_Src = 
			access::placeholder::false_t,
		access::placeholder IsPlaceholder_Dst = 
			access::placeholder::false_t>
	void copy(accessor<T_Src, Dims_Src, AccessMode_Src, 
					AccessTarget_Src, IsPlaceholder_Src>
			Src,
			accessor<T_Dst, Dims_Dst, AccessMode_Dst, 
					AccessTarget_Dst, IsPlaceholder_Dst>
			Dst);
			
	// Provides a guarantee that the memory object accessed by the accessor
	// is updated on the host after this action executes.
	template <typename T, int Dims, 
			access::mode AccessMode,
			access::target AccessTarget,
			access::placeholder IsPlaceholder =
				access::placeholder::false_t>
	void update_host(accessor<T, Dims, AccessMode, 
					AccessTarget, IsPlaceholder> Acc);
	...
};
\end{lstlisting}


## 3.11 总结

本章中，介绍了解决数据管理的机制，以及如何对数据的使用进行排序。当使用加速器时，管理不同内存的访问是一个挑战，有不同的选择来满足我们的需求。\par

我们概述了数据使用之间可能存在的不同类型依赖关系，并描述了如何向队列提供关于这些依赖关系的信息，以便正确地对任务进行排序。\par

本章提供了统一共享内存和缓冲区的概述。我们将在第6章更详细地探讨USM的所有模式和行为。第7章将更深入地探讨缓冲区，包括创建缓冲区和控制其行为的所有不同方法。第8章将回顾控制内核执行顺序和数据移动的队列的调度机制。\par