
# 16 CPU编程

\begin{center}
	\includegraphics[width=0.3\textwidth]{content/chapter-16/images/1}
\end{center}

内核编程最初作为GPU编程的方式。由于是通用内核编程，理解编程风格如何影响代码向CPU的映射就很重要。\par

CPU在过去的几年里不断发展。在2005年左右发生了一个重大的变化，当时增加时钟速度带来的性能的提升减少了。并行性是最受欢迎的解决方案——CPU生产商引入了多核芯片，而不是提高时钟频率。计算机可以同时执行多个任务!\par

虽然多核是提高硬件性能的主流方式，但在软件上释放这种能力需要付出很大的努力。多核处理器要求开发人员对算法进行修改，这样硬件性能增益才会明显，但想要做到却很难。拥有的核芯越多，就越难高效地工作。DPC++是解决这些挑战的编程语言，有助于在CPU(和其他体系结构)上开发各种形式的并行。\par

本章讨论了CPU架构的一些细节，CPU硬件如何执行DPC++应用程序，并提供了在为CPU平台编写DPC++代码的最佳实践。\par


## 16.1 性能说明

DPC++为并行化应用程序铺了一条路。当程序在CPU上运行时，其性能在很大程度上取决于以下因素:\par

\begin{itemize}
	\item 内核代码的调用方式和执行底层的性能
	\item 并行内核中运行代码的百分和可扩展性
	\item CPU利用率、数据共享、数据局部性和负载均衡
	\item 工作项之间同步和通信的数量
	\item 为创建、恢复、管理、挂起、销毁和同步工作项所执行的线程而引入的开销，串行到并行或并行到串行转换的数量会使性能变得更糟
	\item 由共享内存引起的冲突
	\item 共享资源(如内存、写入组合缓冲区和内存带宽)的性能限制
\end{itemize}

此外，与任何处理器类型一样，CPU可能因厂商的不同而不同，甚至因产品的不同而不同。对于CPU的最佳实践，可能不是针对其他CPU或配置的最佳实践。\par

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
要在CPU上实现最佳性能，请尽可能多地了解相应CPU的架构!
\end{tcolorbox}

## 16.2 通用CPU的基础知识
多核CPU的出现和快速发展推动了共享内存并行计算平台的接受度。CPU提供了笔记本电脑、台式机和服务器级的并行计算平台，使得它们无处不在。CPU体系结构的最常见形式是缓存相关的非一致性内存访问(cc-NUMA)，其特征是访问时间不完全一致。甚至许多小型双插槽CPU系统也有这种内存系统。由于处理器中的核心数量和插槽数量不断增加，这种体系结构已占据主导地位。\par

在cc-NUMA的CPU系统中，每个套接字连接到系统中内存的一个子集。缓存相关的交互将所有的套接字粘在一起，并为开发者提供一致的内存视图。这样的内存系统是可扩展的，因为聚合内存带宽随系统中插槽的数量变化。交互的好处是应用程序可以透明地访问系统中的内存，而不管数据放在哪里。然而，这也是有代价的:内存访问数据和指令的延迟不再一致(例如，固定的访问延迟)。延迟取决于数据存储在系统中的位置。数据来自直接连接到代码运行的套接字的内存。最坏的情况下，数据源于里系统中很远的内存，由于cc-NUMA的CPU系统上插槽之间的数量增加，内存访问成本可能会增加。\par

图16-1是一个具有cc-NUMA内存的通用CPU架构。这种简化的系统架构，包含多插槽系统中的核心和内存组件。本章的其余部分，将用来说明对应代码示例的映射。\par

为了获得最佳性能，需要确保了解特定系统的cc-NUMA配置。例如，Intel最近的服务器使用了网状互连架构，其中核心、缓存和内存控制器组织成行和列。理解处理器与内存的连接性对于实现系统的最佳性能非常重要。\par

\hspace*{\fill} \par %插入空行
图16-1 通用多核CPU系统
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/2}
\end{center}

图16-1中的系统有两个插槽，每个插槽有两个核，每个核有四个硬件线程。每个核都有自己的一级(L1)缓存。L1缓存连接到共享的最后一级缓存，后者连接到套接字上的内存系统，套接字内的内存访问延迟是一致的。\par

这两个卡槽通过缓存相互连连接起来。内存分布在整个系统中，但是所有内存都可以从系统中的任何地方访问。当不在运行访问内存的代码时，内存读写延迟不一致，这意味着当访问远程数据时，可能会施加不一致的延迟。然而，互连的关键是一致性。不需要担心跨内存系统的数据不一致(这将是一个功能问题)，只需要担心访问分布式内存系统的方式对性能的影响。\par

CPU中的硬件线程是执行工具。这些是执行指令流的单元(CPU术语中的线程)。图16-1中的硬件线程从0到15连续编号，这是用于简化本章示例的符号。除特别说明外，本章中有关CPU系统的描述均以图16-1中的cc-NUMA系统为参考。\par

## 16.3 SIMD硬件的基础知识

1996年，第一个SIMD(根据Flynn的分类法，Single Instruction, Multiple Data)指令集是在x86架构之上的MMX。从那以后，许多SIMD指令集扩展既遵循Intel架构，也在行业中都广泛的应用。CPU核心通过执行指令来完成它的工作，核心知道如何执行特定的指令是由它实现的指令集(如x86, x86\_64, AltiVec, NEON)和指令集扩展(如SSE, AVX, AVX-512)定义的。指令集扩展添加的许多操作都集中在SIMD指令上。\par

通过使用比处理的数据基本单元更大的寄存器和硬件，SIMD指令允许在单个核上同时执行多个计算。使用512位寄存器，这样可以用一条机器指令执行8个64位计算。\par

\hspace*{\fill} \par %插入空行
图16-2 在CPU硬件线程中执行SIMD
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/3}
\end{center}

图16-2的这个例子可以带来8倍的加速。实际中，可能在达不到8倍，这是瓶颈可能不完全在计算上，也有部分在内存吞吐量上。通常，使用SIMD的性能优势取决于特定的场景。在一些情况下，甚至比更简单的非SIMD等效代码的性能更差。现代处理器上，如果知道何时以及如何应用(或让编译器应用)SIMD，就可以获得可观的收益。与所有性能优化一样，开发者应该在将目标机器投入生产之前，测试其性能收益。本章接下来的章节中有更多关于预期性能提高的细节。\par

具有SIMD单元的cc-NUMA CPU体系结构，构成了多核处理器的基础，可以以五种不同的方式利用指令级并行，如图16-3所示。\par

\hspace*{\fill} \par %插入空行
图16-3 五种并行执行指令的方法
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/4}
\end{center}

图16-3中，指令级并行可以通过标量指令的无序执行实现，也可以通过单个线程中的SIMD (Single Instruction, Multiple Data)数据并行实现。线程级并行可以通过在同一个核或不同规模的多个核上执行多个线程来实现。更具体地说，线程级并行性可以通过以下方式实现:\par

\begin{itemize}
	\item 现代CPU体系结构允许一个核芯同时执行两个或多个线程的指令。
	\item 每个处理器中包含两个或多个多核架构。操作系统将每个执行核心视为具有所有相关执行资源的独立处理器。
	\item 处理器(芯片)级的多处理，可以通过独立的线程来完成。因此，处理器可以在应用中运行线程，在操作系统中运行另一个线程，或者可以在单个应用程序中运行并行线程。
	\item 分布式处理，可以通过在计算机集群上执行由多个线程组成的进程来完成，这些进程通常通过消息传递框架进行通信。
\end{itemize}

为了充分利用多核处理器资源，软件必须将工作负载分布到多个核的方式编写。这种方法利用了线程级并行性或简单的线程化。\par

随着多处理器计算机和具有超线程(HT)技术的多核处理器越来越多，将并行处理技术作为提高性能的标准实践是非常重要的。本章后面的部分将介绍DPC++中的编码方法和性能调优技术，这些技术允许在多核CPU上实现最高性能。\par

与其他并行处理硬件(例如GPU)一样，给CPU足够大的数据元素集来处理很重要。为了说明如何利用多级并行处理大量数据的重要性，请考虑一个简单的C++ STREAM Triad程序，如图16-4所示。\par

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black, title=关于STREAM Triad工作负载的解释]
STREAM Triad工作负载(www.cs.virginia.edu/stream)是一个基准工作负载，CPU供应商使用它来演示高度调优的性能。使用STREAM Triad内核来演示并行内核的代码生成，以及通过本章描述的技术来实现显著提高性能的计划方式。STREAM Triad是一个相对简单的工作负载，但足以显示许多优化。
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black, title=使用供应商提供的库!]
当供应商提供函数库实现时，使用它而不是将函数重新实现为并行内核!
\end{tcolorbox}

\hspace*{\fill} \par %插入空行
图16-4 STREAM Triad C++循环
\begin{lstlisting}[caption={}]
// C++ STREAM Triad workload
// __restrict is used to denote no memory aliasing among arguments 
template <typename T>
double triad(T* __restrict VA, T* __restrict VB, 
			 T* __restrict VC, size_t array_size, const T scalar) {
	double ts = timer_start()
	for (size_t id = 0; id < array_size; id++) {
		VC[id] = VA[id] + scalar * VB[id];
	}
	double te = timer_end();
	return (te – ts); 
}
\end{lstlisting}

STREAM Triad循环可以在CPU上简单地执行，使用单个CPU进行串行执行。好的C++编译器会将执行循环向量化，为具有SIMD硬件的CPU生成SIMD代码，以便利用指令级的SIMD并行性。例如，对于支持AVX-512的Intel Xeon处理器，Intel C++编译器生成SIMD代码，如图16-5所示。编译器对代码的转换减少了执行时的循环迭代次数，这是通过在运行时每个循环迭代执行更多的工作(SIMD宽度和展开的迭代)!\par

\hspace*{\fill} \par %插入空行
图16-5 TREAM Triad C++循环的AVX-512代码
\begin{lstlisting}[caption={}]
// STREAM Triad: SIMD code generated by the compiler, where zmm0, zmm1 
// and zmm2 are SIMD vector registers. The vectorized loop is unrolled by 4
// to leverage the out-of-execution of instructions from Xeon CPU and to
// hide memory load and store latency 

# %bb.0: # %entry
vbroadcastsd %xmm0, %zmm0 # broadcast “scalar” to SIMD reg zmm0
movq $-32, %rax
.p2align 4, 0x90
.LBB0_1: # %loop.19
# =>This Loop Header: Depth=1
vmovupd 256(%rdx,%rax,8), %zmm1 # load 8 elements from memory to zmm1 
vfmadd213pd 256(%rsi,%rax,8), %zmm0, %zmm1 # zmm1=(zmm0*zmm1)+mem
# perform SIMD FMA for 8 data elements 
# VC[id:8] = scalar*VB[id:8]+VA[id:8] 
vmovupd %zmm1, 256(%rdi,%rax,8) # store 8-element result to mem from zmm1 
# This SIMD loop body is unrolled by 4
vmovupd 320(%rdx,%rax,8), %zmm1
vfmadd213pd 320(%rsi,%rax,8), %zmm0, %zmm1 # zmm1=(zmm0*zmm1)+mem
vmovupd %zmm1, 320(%rdi,%rax,8)
vmovupd 384(%rdx,%rax,8), %zmm1
vfmadd213pd 384(%rsi,%rax,8), %zmm0, %zmm1 # zmm1=(zmm0*zmm1)+mem
vmovupd %zmm1, 384(%rdi,%rax,8)
vmovupd 448(%rdx,%rax,8), %zmm1
vfmadd213pd 448(%rsi,%rax,8), %zmm0, %zmm1 # zmm1=(zmm0*zmm1)+mem
vmovupd %zmm1, 448(%rdi,%rax,8)
addq $32, %rax
cmpq $134217696, %rax # imm = 0x7FFFFE0
jb .LBB0_1
\end{lstlisting}

如图16-5所示，编译器能以两种方式利用指令级并行性。首先是通过SIMD指令的使用，利用指令级数据并行性，其中一条指令可以同时并行处理8个双精度数据元素(每个指令)。其次，基于硬件多路指令调度，编译器循环展开来获得(指令之间没有依赖关系)乱序执行是效果。\par

如果尝试在CPU上执行这个函数，可能会运行得很好——没有利用CPU的任何多核或线程能力，对于小数组来说已经足够好了。但是，当试图在CPU上使用大数组来执行这个函数，那性能很可能会很差，因为单个线程只使用一个CPU核，当这个核的内存带宽饱和时就会出现性能瓶颈。\par

## 16.4 利用线程级别的并行性

为了提高STREAM Triad内核在CPU和GPU上的性能，可以通过将循环转换为paralle\_for内核来计算。\par

STREAM Triad内核可以将其提交到队列，并在CPU上并行执行。STREAM Triad DPC++并行内核的主体看起来就像在CPU上串行C++中执行的STREAM Triad循环的主体，如图16-6所示。\par

\hspace*{\fill} \par %插入空行
图16-6 DPC++ STREAM Triad的paralle\_for内核代码
\begin{lstlisting}[caption={}]
constexpr int num_runs = 10;
constexpr size_t scalar = 3;

double triad(
		const std::vector<double>& vecA,
		const std::vector<double>& vecB,
		std::vector<double>& vecC ) {
			
	assert(vecA.size() == vecB.size() == vecC.size());
	const size_t array_size = vecA.size();
	double min_time_ns = DBL_MAX;
	
	queue Q{ property::queue::enable_profiling{} };
	std::cout << "Running on device: " <<
		Q.get_device().get_info<info::device::name>() << "\n";
	
	buffer<double> bufA(vecA);
	buffer<double> bufB(vecB);
	buffer<double> bufC(vecC);
	
	for (int i = 0; i< num_runs; i++) {
		auto Q_event = Q.submit([&](handler& h) {
			accessor A{ bufA, h };
			accessor B{ bufB, h };
			accessor C{ bufC, h };
			
			h.parallel_for(array_size, [=](id<1> idx) {
				C[idx] = A[idx] + B[idx] * scalar;
			});
		});
	
		double exec_time_ns =
			Q_event.get_profiling_info<info::event_profiling::command_end>() -
			Q_event.get_profiling_info<info::event_profiling::command_start>();
		
		std::cout << "Execution time (iteration " << i << ") [sec]: "
				  << (double)exec_time_ns * 1.0E-9 << "\n";
		min_time_ns = std::min(min_time_ns, exec_time_ns);
	}

	return min_time_ns;
}
\end{lstlisting}

尽管并行内核非常类似于用带有循环的串行STREAM Triad函数，但在CPU上运行速度要快得多，因为paralle\_for允许在多个内核上并行处理数组的不同元素。如图16-7所示，假设有一个系统，其中有一个插槽、四个内核和每个内核有两个超线程，有1024个双精度数据元素要处理。现实中，数据在包含32个数据元素的工作组中处理，这意味着有8个线程和32个工作组。工作组调度可以按循环顺序进行，即thread-id = work-group-id mod 8。每个线程将执行四个工作组，每一轮可以并行执行8个工作组。注意，在本例中工作组是DPC++编译器和运行时会隐式形成的。\par

\hspace*{\fill} \par %插入空行
图16-7 STREAM Triad内核并行的映射
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/5}
\end{center}

注意，在DPC++程序中不需要指定数据元素的分区，以及相应的处理器。这使得DPC++可以灵活地选择，如何在特定的CPU上最好地并行内核。话虽如此，实现可以为程序员提供某种程度的控制，以便性能调优。\par

虽然CPU可能会带来相对较高的线程上下文切换和同步开销，但在处理器上开辟更多的线程是有益的，因为每个处理器核心提供了要执行的工作的选择。如果软件线程正在等待另一个线程产生数据，那么处理器可以切换到另一个准备运行的软件线程，而不会让处理器空闲。\par

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black, title=选择如何绑定和调度线程]
选择一种有效的方案来划分和调度线程之间的工作，对于在CPU和其他设备类型上调优非常重要。接下来的将描述这部分的技术。
\end{tcolorbox}

\hspace*{\fill} \par %插入空行
\textbf{线程的亲和力}

线程亲和力指定特定线程可以在特定的CPU上执行。如果线程在多个核之间移动，性能可能会受到影响，例如：如果线程不在同一个核上执行，如果数据在核之间打乒乓球，则缓存的性能会变得很低。\par

DPC++运行时库支持通过环境变量DPCPP\_CPU\_CU\_AFFINITY、DPCPP\_CPU\_PLACES、DPCPP\_CPU\_NUM\_CUS和DPCPP\_CPU\_SCHEDULE等将线程绑定到内核，但这都不是由SYCL定义。\par

首先是环境变量DPCPP\_CPU\_CU\_AFFINITY。使用这些环境变量进行简单调优，并且对许多应用程序有很大影响。这个环境变量的描述如图16-8所示。\par

\hspace*{\fill} \par %插入空行
图16-8 DPCPP\_CPU\_CU\_AFFINITY环境变量
\begin{table}[H]
	\begin{tabular}{|l|l|}
		\hline
		\textbf{DPCPP\_CPU\_CU\_AFFINITY} & \textbf{描述}                                                                                                                    \\ \hline
		spread                            & \begin{tabular}[c]{@{}l@{}}按照循环顺序将连续线程绑定到从插槽0开始\end{tabular}      \\ \hline
		close                             & \begin{tabular}[c]{@{}l@{}}以循环顺序将连续线程绑定到不同的超线程(以线程0开始)\end{tabular} \\ \hline
	\end{tabular}
\end{table}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black]
spread: boundHT = (tid mod numHT) + ( tid mod numSocket) x numHT \\
close: boundHT = tid mod(numSocket x numHT)
\end{tcolorbox}

\begin{itemize}
	\item tid示软件线程标识符。
	\item boundHT表示线程tid绑定到的超线程(逻辑核芯)。
	\item numHT表示每个插槽的超线程数。
	\item numSocket表示系统中的插槽数量
\end{itemize}

假设在一个双核双插槽的超线程系统上，运行一个有8个线程的程序——换句话说，有4个核，总共有8个超线程要进行编程。图16-9展示了线程如何映射到不同DPCPP\_CPU\_CU\_AFFINITY设置的超线程和核芯。\par

\hspace*{\fill} \par %插入空行
图16-9 用超线程将线程映射到内核
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/6}
\end{center}

除了环境变量DPCPP\_CPU\_CU\_AFFINITY，还有其他支持CPU性能调优的环境变量:\par

\begin{itemize}
	\item DPCPP\_CPU\_NUM\_CUS = [n]，它设置用于内核执行的线程数。它的默认值是系统中的硬件线程数。
	\item DPCPP\_CPU\_PLACES = [ sockets | numa\_domains | 	cores | threads ]，指定亲和性将被设置的位置，类似于OpenMP 5.1中的OMP\_PLACES。默认设置为“cores”。
	\item DPCPP\_CPUSCHEDULE = [ dynamic | affinity | static ]，它指定了调度工作组的算法。默认设置是dynamic。
	\begin{itemize}
		\item dynamic: 启用TBB auto\_partitioner，通常可以平衡工作线程之间的负载。
		\item affinity: 启用TBB affinity\_partitioner，可以改进缓存亲和性，并在将子范围映射到工作线程时使用相应的比例分割。
		\item static: 启用TBB static\_partitioner，尽可能均匀地分布线程之间的负载。
	\end{itemize}
\end{itemize}

TBB使用粒度大小来控制工作拆分，默认粒度大小为1，表示所有工作组都可以独立执行。更多信息可以在spec.oneapi.com/versions/latest/elements/oneTBB/source/algorithms.html\#partitioner找到。\par

缺乏线程关联性调优并不一定性能会低。性能更多地取决于并行执行的线程总数，而不是线程和数据的关联和绑定程度。使用基准测试程序，是确定线程关联是否对性能有影响的一种方法。如图16-1所示，DPC++ STREAM Triad代码在没有线程关联设置的情况下以较低的性能启动。通过控制亲和性设置和通过环境变量(输出如下所示)对软件线程进行静态调度，性能得到了改善:\par

\begin{tcolorbox}[colback=white,colframe=black]
export DPCPP\_CPU\_PLACES=numa\_domains\\
export DPCPP\_CPU\_CU\_AFFINITY=close
\end{tcolorbox}

通过使用numa\_domains对亲和性的位置进行设置，TBB任务领域绑定到numa节点和插槽，并且任务均匀地分布在各个领域。一般情况下，环境变量DPCPP\_CPU\_PLACES建议与DPCPP\_CPU\_CU\_AFFINITY一起使用。这些环境变量设置可以在拥有2个插槽和28个双向超线程内核的Skylake服务器系统上实现了30\%的性能提升，每个插槽中的核芯以2.5GHz运行。但是，我们还可以做得更好，可以进一步提高这个CPU的性能。\par

\hspace*{\fill} \par %插入空行
\textbf{注意与内存的接触}

示例中，初始化循环不是并行的，由主机线程串行执行，所有内存都与主机线程运行的任务相关联。其他任务随后的访问将连接到初始任务(用于初始化)的内存中的数据，这显然不符合性能要求。通过并行化初始化循环来控制跨任务与内存的第一次接触，可以在STREAM Triad内核上实现更高的性能，如图16-10所示。\par

\hspace*{\fill} \par %插入空行
图16-10 STREAM Triad并行初始化内核控制与内存的第一次接触
\begin{lstlisting}[caption={}]
template <typename T>
void init(queue &deviceQueue, T* VA, T* VB, T* VC, size_t array_size) {
	range<1> numOfItems{array_size};
	
	buffer<T, 1> bufferA(VA, numOfItems);
	buffer<T, 1> bufferB(VB, numOfItems);
	buffer<T, 1> bufferC(VC, numOfItems);
	
	auto queue_event = deviceQueue.submit([&](handler& cgh) {
		auto aA = bufA.template get_access<sycl_write>(cgh);
		auto aB = bufB.template get_access<sycl_write>(cgh);
		auto aC = bufC.template get_access<sycl_write>(cgh);
		
		cgh.parallel_for<class Init<T>>(numOfItems, [=](id<1> wi) {
			aA[wi] = 2.0; aB[wi] = 1.0; aC[wi] = 0.0;
		});
	});

	queue_event.wait();
}
\end{lstlisting}

初始化代码中利用并行性可以提高内核在CPU上运行时的性能。在Intel Xeon处理器系统上，这样可以获得了2倍的性能增益。\par

本章最近的几节展示了通过利用线程级并行性，可以有效地利用CPU内核和超线程。然而，还需要利用CPU核心硬件中的SIMD向量级并行性，以实现最佳性能。\par

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
DPC++并行内核得益于跨核和超线程的线程级并行!
\end{tcolorbox}


## 16.5 CPU上的SIMD向量化

编写良好的DPC++内核没有工作项依赖关系，就可以在CPU上高效地并行，还可以对DPC++内核应用向量化，以利用SIMD硬件。实际上，CPU处理器可以使用SIMD指令优化内存负载、存储和操作，因为大多数数据元素通常位于连续内存中，并且通过数据并行内核采用相同的控制流。例如，有a[i] = a[i] + b[i]语句的内核中，通过在多个数据元素之间共享硬件逻辑，并将它们作为一个组执行，每个数据元素都以相同的指令流load、load、add和store操作，可以自然地映射到硬件的SIMD指令集。因此，一个指令可以同时处理多个数据元素。\par

由一条指令同时处理的数据元素的数量，有时称为指令或执行指令的处理器的向量长度(或SIMD宽度)。图16-11中，指令流以4路SIMD执行运行。\par

\hspace*{\fill} \par %插入空行
图16-11 SIMD的指令流
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/10}
\end{center}

CPU处理器并不是唯一实现SIMD指令集的处理器。其他处理器(如GPU)实现SIMD指令以提高处理大型数据集的效率。与其他处理器类型相比，Intel Xeon CPU处理器的区别是有三个固定大小的SIMD寄存器宽度(128位XMM、256位YMM和512位ZMM)，而不是一个可变长度的SIMD。当使用子工作组或向量类型编写具有SIMD并行性的DPC++代码时，需要注意硬件中的SIMD宽度和SIMD向量寄存器的数量。\par

\hspace*{\fill} \par %插入空行
\textbf{确保SIMD执行的合法性}

DPC++执行模型确保SIMD执行可以应用于任何内核，以及每个工作组中的一组工作项(即一个子工作组)可以使用SIMD指令并发执行。有些实现可以选择使用SIMD指令在内核中执行循环，只有保留所有原始数据依赖关系，或者保留由编译器基于语义解析的数据依赖关系才有可能。\par

使用工作组内的SIMD指令，可以将单个DPC++内核执行从单个工作项的处理转换为一组工作项。在ND-Range模型下，生成SIMD代码的编译器向量器选择了增长最快的(单位步幅)维度。实际上，要启用给定ND-Range的向量化，在同一子工作组中的任何两个工作项之间，不应该存在依赖关系，或者编译器需要在同一子工作组中保留工作项向前依赖关系。\par

当工作项的内核执行映射到CPU上的线程时，细粒度同步的代价很高，线程上下文切换的开销也很高。因此，为CPU编写DPC++内核时，消除工作组内工作项之间的依赖对性能优化很重要。另一种有效的方法是依赖子工作组中的工作项，如图16-12中“先读后写”的依赖。如果子工作组是在SIMD执行模型下执行的，那么编译器可以将内核中的子工作组栅栏忽略，在运行时不会产生同步成本。\par

\hspace*{\fill} \par %插入空行
图16-12 使用子工作组向量化具有前向依赖性的循环
\begin{lstlisting}[caption={}]
using namespace sycl::intel;

queue Q;
range<2> G = {n, w};
range<2> L = {1, w};

int *a = malloc_shared<int>(n*(n+1), Q);

for (int i = 0; i < n; i++)
	for (int j = 0; j < n+1; j++) a[i*n + j] = i + j;
	
Q.parallel_for(nd_range<2>{G, L}, [=](nd_item<2> it)
	[[cl::intel_reqd_sub_group_size(w)]] {
		
	// distribute uniform "i" over the sub-group with 8-way
	// redundant computation
	const int i = it.get_global_id(0);
	sub_group sg = it.get_sub_group();
	
	for (int j = sg.get_local_id()[0]; j < n; j += w) {
		// load a[i*n+j+1:8] before updating a[i*n+j:8] to preserve
		// loop-carried forward dependence
		auto va = a[i*n + j + 1];
		sg.barrier();
		a[i*n + j] = va + i + 2;
	}
	sg.barrier();
	
}).wait();
\end{lstlisting}

内核向量化(向量长度为8)，其SIMD执行如图16-13所示。工作组的规模为(1,8)，内核的循环迭代分布在这些子工作组工作项上，并以8路SIMD并行方式执行。\par

本例中，如果内核中的循环控制性能，那么允许跨子工作组的SIMD向量化将有显著的性能改进。\par

使用并行处理数据元素的SIMD指令，是让内核的性能超出CPU内核和超线程数量的方法。\par

\hspace*{\fill} \par %插入空行
图16-13 具有前向依赖关系的循环的SIMD向量化
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/7}
\end{center}

\hspace*{\fill} \par %插入空行
\textbf{SIMD掩码和成本}

实际应用中，我们可以期待条件语句，如if语句，条件表达式，如a = b > a?A: b，迭代次数可变的循环，switch语句等等。任何条件可能导致标量控制流不执行相同的代码路径，就像在GPU(第15章)，可能会导致性能下降。SIMD掩码是一组值为1或0的位，由内核中的条件语句生成。考虑一个例子，A={1,2,3,4}， B={3,7,8,1}，以及比较表达式A < B。比较返回一个掩码，包含四个值{1,1,1,0}，可以存储在硬件掩码寄存器中，以指示以后哪些通道的SIMD指令应该执行比较所保护(使能)的代码。\par

内核包含条件代码，与基于与每个数据元素相关联的掩码位(SIMD指令中的通道)执行向量化的掩码指令。每个数据元素的掩码位与掩码寄存器中的位置相对应。\par

使用掩码可能会导致性能低于相应的非掩码代码。这可能是由于以下原因：\par

\begin{itemize}
	\item 负载上附加了掩码操作
	\item 对目标的依赖
\end{itemize}

掩码是有成本的，所以只在必要时使用。当内核是具有执行范围内工作项显式分组的ND-Range内核时，在选择ND-Range工作组大小，通过最小化掩码成本来最大限度地提高SIMD效率时应格外小心。当工作组的大小不能被处理器的SIMD宽度均匀整除时，工作组的一部分可能会被内核忽略。\par

\hspace*{\fill} \par %插入空行
图16-14 内核中使用的三种掩码
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/11}
\end{center}

图16-14显示了如何使用合并掩码创建一个依赖于目标寄存器:\par

\begin{itemize}
	\item 如果没有掩码，处理器每个周期执行两个乘法(vmulps)。
	\item 合并掩码时，处理器每四个周期执行两次乘法，因为乘法指令(vmulps)保存在目的寄存器中，如图16-17所示。
	\item 零屏蔽不依赖于目标寄存器，因此在每个周期执行两个乘法(vmulps)。
\end{itemize}

访问缓存对齐的数据比访问非对齐的数据具有更好的性能。地址在编译时未知，或者是未对齐的，这种情况下，可以剥离对内存的访问，使用掩码访问处理前几个元素，直到第一个对齐的地址，然后通过并行内核中的多版本控制技术处理未掩码的访问，随后是忽略剩余数。这种方法增加了代码大小，但从整体上改善了数据处理性能。\par

\hspace*{\fill} \par %插入空行
\textbf{避免使用结构数组来提高SIMD效率}

AOS(结构数组)结构会影响SIMD的效率，也会为内存访问带来额外的带宽和延迟。硬件聚集-分散机制的存在并不能消除这种转换的需求——聚集-分散访问通常需要比连续负载更高的带宽和延迟。给定一个AOS数据布局struct \{float x; float y; float z; float w;\} a[4]，考虑一个内核在它上面操作，如图16-15所示。\par

\hspace*{\fill} \par %插入空行
图16-15 SIMD在内核中聚集
\begin{lstlisting}[caption={}]
cgh.parallel_for<class aos<T>>(numOfItems,[=](id<1> wi) {
	x[wi] = a[wi].x; // lead to gather x0, x1, x2, x3
	y[wi] = a[wi].y; // lead to gather y0, y1, y2, y3
	z[wi] = a[wi].z; // lead to gather z0, z1, z2, z3 
	w[wi] = a[wi].w; // lead to gather w0, w1, w2, w3
});
\end{lstlisting}

当编译器沿着一组工作项向量化内核时，由于是非单位步长的内存访问，会导致SIMD收集指令的生成。例如，a[0].x, a[1].x, a[2].x和a[3].x的跨距是4,，不是更有效的步幅1。\par

\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/8}
\end{center}

内核中，通常可以通过消除对内存收集-分散操作的使用来实现更高效的SIMD。一些代码可以从数据布局中获益，这些将以结构数组(Array-of-Struct, AOS)表示形式编写的数据结构转换为阵列结构(Structure of Arrays, SOA)，在执行SIMD向量化时，为每个结构字段使用单独的数组以保持内存访问的连续性。例如，考虑这样一个SOA数据布局:struct {float x[4]; float y[4]; float z[4]; float w[4];} a; 如下所示:\par

\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/9}
\end{center}

内核可以使用如图16-16所示的单位步长(连续)向量加载和存储数据，即使是向量化!

\hspace*{\fill} \par %插入空行
图16-16 SIMD操作单位跨距的内核
\begin{lstlisting}[caption={}]
cgh.parallel_for<class aos<T>>(numOfItems,[=](id<1> wi) {
	x[wi] = a.x[wi]; // lead to unit-stride vector load x[0:4]
	y[wi] = a.y[wi]; // lead to unit-stride vector load y[0:4]
	z[wi] = a.z[wi]; // lead to unit-stride vector load z[0:4]
	w[wi] = a.w[wi]; // lead to unit-stride vector load w[0:4]
});
\end{lstlisting}

SOA数据布局有助于在跨数组元素访问结构的一个字段时防止聚集，并帮助编译器对与工作项相关的连续数组元素上的内核进行向量化。考虑到使用这些数据结构的地方，希望在程序级别完成这些AOS-to-SOA或AOSOA的数据布局转换。仅在循环级别上执行此操作将涉及循环前后格式之间的转换。我们还可以依赖编译器对AOS数据布局进行向量加载和混洗优化，但要付出一定的代价。如果SOA(或AOS)数据布局的成员具有向量类型，那么编译器向量化将执行第11章中描述的基于底层硬件的水平扩展或垂直扩展，以生成最佳代码。\par

\hspace*{\fill} \par %插入空行
\textbf{数据类型对SIMD效率的影响}

只要C++开发者知道数据适合32位带符号的类型，就会使用整数数据类型，经常出现以下的代码\par

\begin{tcolorbox}[colback=white,colframe=black]
int id = get\_global\_id(0); \\a[id] = b[id] + c[id];
\end{tcolorbox}

假设get\_global\_id(0)的返回类型是size\_t(无符号整数，通常是64位)，这种转换降低了编译器的可优化性。

\begin{itemize}
	\item 读取[get\_global\_id(0)]会导致SIMD单位跨距的向量负载。
	\item 读取[(int)get\_global\_id(0)]会导致形成非单元跨距的收集指令。
\end{itemize}

这种微妙的情况是由从size\_t到int(或uint)的数据类型转换行为(未指定的行为C/C++标准中定义良好的转换行为)引起的，这主要是基于C语言进化的历史产物。具体来说，某些转换中的溢出是未定义的行为，这实际上允许编译器假定这种情况永远不会发生，并更积极地进行优化。图16-17显示了一些希望了解细节的示例。\par

\hspace*{\fill} \par %插入空行
图16-17 整型值范围
\begin{center}
	\includegraphics[width=1.0\textwidth]{content/chapter-16/images/12}
\end{center}

SIMD收集/分散指令比SIMD单元跨矢量加载/存储操作慢。为了最佳的SIMD效率，避免聚集/分散对于应用程序来说是至关重要的，无论使用哪种编程语言。\par

大多数SYCL get\_*\_id()函数有相同的问题，尽管许多情况下适用于MAX\_INT，因为返回值是有界的(例如，一个工作组中的最大id)。只要是合法的，DPC++编译器就会假定跨相邻工作项的单元跨内存地址，以避免聚集/分散。由于全局id和/或全局id的值可能溢出，编译器无法安全地生成单位跨距向量内存加载/存储操作，编译器将生成聚集/散点操作。\par

为用户提供最佳性能的理念下，DPC++编译器假定没有溢出，并且在实践中捕获真实状态，因此编译器可以生成最佳的SIMD代码以获得良好的性能。D\_\_SYCL\_DISABLE\_ID\_TO\_INT\_CONV\_\_是DPC++编译器用来告诉编译器会发生溢出的宏，使用id查询向量的访问可能不安全，这可能会对性能产生很大影响。这个宏应该在不安全的情况下使用，以假定没有溢出发生。\par

\hspace*{\fill} \par %插入空行
\textbf{使用single\_task执行SIMD}

单个任务执行模型中，向量类型和函数相关的优化依赖于编译器。编译器和运行时可以自由地支持显式SIMD执行或在single\_task内核中选择标量执行，结果取决于编译器实现。例如，DPC++ CPU编译器将为向量类型生成SIMD指令。vec加载、存储和swizzle函数将直接对向量变量执行操作，通知编译器数据元素正从内存中的相同(统一)位置开始访问连续数据，并能够请求优化的连续数据加载/存储。\par

\hspace*{\fill} \par %插入空行
图16-18 single\_task内核中使用向量类型和混合操作
\begin{lstlisting}[caption={}]
queue Q;
bool *resArray = malloc_shared<bool>(1, Q);
resArray[0] = true;

Q.single_task([=](){
	sycl::vec<int, 4> old_v = sycl::vec<int, 4>(000, 100, 200, 300);
	sycl::vec<int, 4> new_v = sycl::vec<int, 4>();
	
	new_vb.rgba() = old_v.abgr();
	int vals[] = (300, 200, 100, 000);
	
	if (new_v.r() != vals[0] || new_v.g() != vals[1] ||
	    new_v.b() != vals[2] || new_v.a() != vals[3]) {
      resArray[0] = false;    
    }
}).wait();
\end{lstlisting}

如图16-18所示，执行单个任务时，声明了一个包含三个数据元素的vector。使用old\_v.abgr()执行混合操作。如果CPU为混合操作提供SIMD硬件指令，那么可以通过在程序中使用混合操作获得一些性能收益。\par

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black, title=SIMD向量化指南]
CPU处理器用不同的SIMD宽度实现SIMD指令集。这是一个实现细节，并且对在CPU上执行内核的应用程序是透明的，因为编译器可以确定要用特定的SIMD大小处理的一组有效的数据元素，而不要求显式地使用SIMD指令。子工作组可以更直接地表示数据元素的分组，应受内核中SIMD执行的影响。\\

考虑到计算复杂度，选择最适合向量化的代码和数据布局最终可能会带来更高的性能收益。选择数据结构时，尽量选择数据布局、对齐方式和数据宽度，使最频繁执行的计算能够以对SIMD友好的方式访问内存，并具有最大的并行性。
\end{tcolorbox}


## 16.6 总结

为了充分利用CPU上的线程级并行性和SIMD向量级并行性，需要牢记以下目标:\par

\begin{itemize}
	\item 熟悉所有类型的DPC++并行性和相应CPU的底层架构。
	\item 在最匹配硬件资源的线程级别上，不要增加或减少正确的并行度。使用供应商工具，如调试器和分析器，来帮助指导我们的调优工作，以实现这一点。
	\item 首先要注意线程关联性和内存对程序性能的影响。
	\item 使用数据布局、对齐和数据宽度设计数据结构，以便最频繁执行的计算能以SIMD友好的方式访问内存，并具有最大的SIMD并行性。
	\item 要注意平衡掩码和代码分支的成本。
	\item 使用清晰的编程风格，最大限度地减少潜在的内存混叠和副作用。
	\item 注意使用向量类型和接口的可扩展性限制。如果编译器将它们映射到SIMD指令，那么跨多代CPU和来自不同供应商的CPU的固定向量大小，可能无法很好地与SIMD寄存器宽度适配。
\end{itemize}

